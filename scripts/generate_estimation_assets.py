#!/usr/bin/env python3
"""Extract estimation artefacts for the faithfulness whitepaper."""

from __future__ import annotations

import argparse
import json
import os
from typing import Dict, List, Any


def load_json(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def write_csv(path: str, header: List[str], rows: List[List[Any]]) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(",".join(header) + "\n")
        for row in rows:
            f.write(",".join(str(item) for item in row) + "\n")


def _fmt_float(value: Any) -> str:
    if value is None or value == "":
        return ""
    try:
        return f"{float(value):.2f}"
    except (TypeError, ValueError):
        return str(value)


def build_cv_folds_table(cv_report: Dict[str, Any]) -> List[List[str]]:
    selected = (cv_report.get("cv") or {}).get("selected") or {}
    fold_metrics = selected.get("fold_metrics") or []
    rows: List[List[str]] = []
    for idx, metrics in enumerate(fold_metrics, start=1):
        rows.append([
            idx,
            _fmt_float(metrics.get("f1")),
            _fmt_float(metrics.get("precision")),
            _fmt_float(metrics.get("recall")),
            _fmt_float(metrics.get("pr_auc")),
            _fmt_float(metrics.get("roc_auc")),
            _fmt_float(metrics.get("brier")),
            _fmt_float(metrics.get("thr")),
        ])
    return rows


def build_answer_type_table(cv_report: Dict[str, Any]) -> List[List[str]]:
    test = cv_report.get("test") or {}
    by_type = test.get("by_answer_type") or {}
    rows: List[List[str]] = []
    for answer_type, metrics in sorted(by_type.items()):
        rows.append([
            answer_type or "unknown",
            int(metrics.get("support", 0)),
            _fmt_float(metrics.get("precision")) if metrics.get("precision") is not None else "",
            _fmt_float(metrics.get("recall")) if metrics.get("recall") is not None else "",
            _fmt_float(metrics.get("f1")) if metrics.get("f1") is not None else "",
            _fmt_float(metrics.get("pr_auc")) if metrics.get("pr_auc") is not None else "",
        ])
    return rows


def build_summary(cv_report: Dict[str, Any]) -> Dict[str, Any]:
    cv = cv_report.get("cv") or {}
    selected = cv.get("selected") or {}
    summary = {
        "bestC": cv_report.get("bestC"),
        "final_threshold": cv_report.get("final_threshold"),
        "n_train": cv_report.get("n_train"),
        "cv_objective": cv.get("objective"),
        "cv_n_splits": cv.get("n_splits"),
        "cv_seed": cv.get("seed"),
        "cv_beta": cv.get("beta"),
        "cv_min_precision": cv.get("min_precision"),
        "cv_mean_metrics": selected.get("mean_metrics"),
        "test_metrics": cv_report.get("test"),
    }
    return summary


def write_summary_tex(path: str, summary: Dict[str, Any]) -> None:
    def _fmt(value: Any) -> str:
        if value is None or value == "":
            return "n/a"
        if isinstance(value, float):
            return f"{value:.2f}"
        return str(value)

    lines = ["% Auto-generated by generate_estimation_assets.py"]
    lines.append(f"\\newcommand{{\\EstBestC}}{{{_fmt(summary.get('bestC'))}}}")
    lines.append(f"\\newcommand{{\\EstFinalThreshold}}{{{_fmt(summary.get('final_threshold'))}}}")
    lines.append(f"\\newcommand{{\\EstTrainSize}}{{{_fmt(summary.get('n_train'))}}}")
    lines.append(f"\\newcommand{{\\EstCvNSplits}}{{{_fmt(summary.get('cv_n_splits'))}}}")
    lines.append(f"\\newcommand{{\\EstCvObjective}}{{{_fmt(summary.get('cv_objective'))}}}")
    lines.append(f"\\newcommand{{\\EstCvSeed}}{{{_fmt(summary.get('cv_seed'))}}}")
    lines.append(f"\\newcommand{{\\EstCvBeta}}{{{_fmt(summary.get('cv_beta'))}}}")
    lines.append(f"\\newcommand{{\\EstCvMinPrecision}}{{{_fmt(summary.get('cv_min_precision'))}}}")

    cv_metrics = (summary.get("cv_mean_metrics") or {})
    lines.append(f"\\newcommand{{\\EstCvFOne}}{{{_fmt(cv_metrics.get('f1'))}}}")
    lines.append(f"\\newcommand{{\\EstCvPrecision}}{{{_fmt(cv_metrics.get('precision'))}}}")
    lines.append(f"\\newcommand{{\\EstCvRecall}}{{{_fmt(cv_metrics.get('recall'))}}}")
    lines.append(f"\\newcommand{{\\EstCvPrauc}}{{{_fmt(cv_metrics.get('pr_auc'))}}}")
    lines.append(f"\\newcommand{{\\EstCvRocauc}}{{{_fmt(cv_metrics.get('roc_auc'))}}}")
    lines.append(f"\\newcommand{{\\EstCvBrier}}{{{_fmt(cv_metrics.get('brier'))}}}")

    test_metrics = summary.get("test_metrics") or {}
    lines.append(f"\\newcommand{{\\EstTestFOne}}{{{_fmt(test_metrics.get('f1'))}}}")
    lines.append(f"\\newcommand{{\\EstTestPrecision}}{{{_fmt(test_metrics.get('precision'))}}}")
    lines.append(f"\\newcommand{{\\EstTestRecall}}{{{_fmt(test_metrics.get('recall'))}}}")
    lines.append(f"\\newcommand{{\\EstTestPrauc}}{{{_fmt(test_metrics.get('pr_auc'))}}}")
    lines.append(f"\\newcommand{{\\EstTestRocauc}}{{{_fmt(test_metrics.get('roc_auc'))}}}")
    lines.append(f"\\newcommand{{\\EstTestBrier}}{{{_fmt(test_metrics.get('brier'))}}}")

    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines) + "\n")


def _latex_monospace(text: str) -> str:
    if text is None:
        return ""
    replacements = {
        "\\": r"\textbackslash{}",
        "_": r"\_",
        "%": r"\%",
        "#": r"\#",
        "&": r"\&",
        "$": r"\$",
    }
    out = ""
    for ch in str(text):
        out += replacements.get(ch, ch)
    return f"\\texttt{{{out}}}"


def top_feature_rows(feature_report: Dict[str, Any], direction: str, limit: int = 5) -> List[List[Any]]:
    key = "top_positive_coefficients" if direction == "positive" else "top_negative_coefficients"
    items = feature_report.get(key) or []
    rows: List[List[Any]] = []
    for rank, entry in enumerate(items[:limit], start=1):
        if isinstance(entry, dict):
            feature = entry.get("feature")
            coefficient = entry.get("coefficient")
        else:
            feature, coefficient = entry[0], entry[1]
        rows.append([
            direction,
            rank,
            _latex_monospace(feature),
            _fmt_float(coefficient),
        ])
    return rows


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Produce estimation artefacts for the whitepaper.")
    parser.add_argument("--cv", required=True, help="Path to cv_report.json")
    parser.add_argument("--features", required=True, help="Path to feature_report.json")
    parser.add_argument("--out-dir", default="report/files", help="Output directory for generated files")
    parser.add_argument("--positive-limit", type=int, default=5, help="How many top positive coefficients to include")
    parser.add_argument("--negative-limit", type=int, default=5, help="How many top negative coefficients to include")
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    cv_report = load_json(args.cv)
    feature_report = load_json(args.features)

    # CV folds table
    cv_rows = build_cv_folds_table(cv_report)
    write_csv(
        os.path.join(args.out_dir, "estimation_cv_folds.csv"),
        ["Fold", "F1", "Precision", "Recall", "PR_AUC", "ROC_AUC", "Brier", "Threshold"],
        cv_rows,
    )

    # Answer-type table
    answer_rows = build_answer_type_table(cv_report)
    write_csv(
        os.path.join(args.out_dir, "estimation_test_answer_type.csv"),
        ["AnswerType", "Support", "Precision", "Recall", "F1", "PR_AUC"],
        answer_rows,
    )

    # Summary JSON
    summary = build_summary(cv_report)
    os.makedirs(args.out_dir, exist_ok=True)
    summary_path = os.path.join(args.out_dir, "estimation_summary.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2)
    write_summary_tex(os.path.join(args.out_dir, "estimation_summary.tex"), summary)

    # Top features
    top_rows: List[List[Any]] = []
    top_rows.extend(top_feature_rows(feature_report, "positive", args.positive_limit))
    top_rows.extend(top_feature_rows(feature_report, "negative", args.negative_limit))
    write_csv(
        os.path.join(args.out_dir, "estimation_top_features.csv"),
        ["Direction", "Rank", "Feature", "Coefficient"],
        top_rows,
    )

    print("Generated estimation artefacts:")
    print("  - estimation_cv_folds.csv")
    print("  - estimation_test_answer_type.csv")
    print("  - estimation_summary.json")
    print("  - estimation_summary.tex")
    print("  - estimation_top_features.csv")


if __name__ == "__main__":
    main()
